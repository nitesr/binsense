{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/nitesr/binsense/blob/FinalFinetunhood_augmentation/Copy_of_FinalFinetunhood.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "JD1xZwnJ6yUL",
        "outputId": "8779a9fc-ac0b-42b8-ed31-b848f3ecf418",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: accelerate==0.27.2 in /usr/local/lib/python3.10/dist-packages (0.27.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.27.2) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.27.2) (24.0)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.27.2) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==0.27.2) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.27.2) (2.2.1+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate==0.27.2) (0.22.2)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate==0.27.2) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.13.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (4.11.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (3.1.3)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (2.19.3)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (12.1.105)\n",
            "Requirement already satisfied: triton==2.2.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate==0.27.2) (2.2.0)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate==0.27.2) (12.4.127)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.27.2) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate==0.27.2) (4.66.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate==0.27.2) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate==0.27.2) (2024.2.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate==0.27.2) (1.3.0)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.10/dist-packages (4.38.2)\n",
            "Requirement already satisfied: evaluate in /usr/local/lib/python3.10/dist-packages (0.4.1)\n",
            "Requirement already satisfied: datasets in /usr/local/lib/python3.10/dist-packages (2.19.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from transformers) (3.13.4)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.22.2)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers) (24.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (6.0.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers) (2023.12.25)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from transformers) (2.31.0)\n",
            "Requirement already satisfied: tokenizers<0.19,>=0.14 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.15.2)\n",
            "Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.10/dist-packages (from transformers) (0.4.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.10/dist-packages (from transformers) (4.66.2)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.3.8)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from evaluate) (2.0.3)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from evaluate) (3.4.1)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.70.16)\n",
            "Requirement already satisfied: fsspec[http]>=2021.05.0 in /usr/local/lib/python3.10/dist-packages (from evaluate) (2023.6.0)\n",
            "Requirement already satisfied: responses<0.19 in /usr/local/lib/python3.10/dist-packages (from evaluate) (0.18.0)\n",
            "Requirement already satisfied: pyarrow>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets) (14.0.2)\n",
            "Requirement already satisfied: pyarrow-hotfix in /usr/local/lib/python3.10/dist-packages (from datasets) (0.6)\n",
            "Requirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets) (3.9.5)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (23.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (6.0.5)\n",
            "Requirement already satisfied: yarl<2.0,>=1.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (1.9.4)\n",
            "Requirement already satisfied: async-timeout<5.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets) (4.0.3)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.11.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->transformers) (2024.2.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas->evaluate) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->evaluate) (1.16.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate==0.27.2\n",
        "!pip install transformers evaluate datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "eBq4mm7cA4Cp",
        "outputId": "70b034f2-258e-48e4-d107-5efc46acafa2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/transformers/deepspeed.py:23: FutureWarning: transformers.deepspeed module is deprecated and will be removed in a future version. Please import deepspeed modules directly from transformers.integrations\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import GenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_tf_utils.py:24: FutureWarning: Importing `TFGenerationMixin` from `src/transformers/generation_tf_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import TFGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/generation_flax_utils.py:24: FutureWarning: Importing `FlaxGenerationMixin` from `src/transformers/generation_flax_utils.py` is deprecated and will be removed in Transformers v4.40. Import as `from transformers import FlaxGenerationMixin` instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/config.json\n",
            "text_config is None. Initializing the OwlViTTextConfig with default values.\n",
            "vision_config is None. initializing the OwlViTVisionConfig with default values.\n",
            "Model config OwlViTConfig {\n",
            "  \"_name_or_path\": \"google/owlvit-base-patch32\",\n",
            "  \"architectures\": [\n",
            "    \"OwlViTForObjectDetection\"\n",
            "  ],\n",
            "  \"initializer_factor\": 1.0,\n",
            "  \"logit_scale_init_value\": 2.6592,\n",
            "  \"model_type\": \"owlvit\",\n",
            "  \"projection_dim\": 512,\n",
            "  \"text_config\": {\n",
            "    \"bos_token_id\": 0,\n",
            "    \"dropout\": 0.0,\n",
            "    \"eos_token_id\": 2,\n",
            "    \"max_length\": 16,\n",
            "    \"model_type\": \"owlvit_text_model\",\n",
            "    \"pad_token_id\": 1\n",
            "  },\n",
            "  \"text_config_dict\": null,\n",
            "  \"torch_dtype\": \"float32\",\n",
            "  \"transformers_version\": \"4.38.2\",\n",
            "  \"vision_config\": {\n",
            "    \"dropout\": 0.0,\n",
            "    \"model_type\": \"owlvit_vision_model\"\n",
            "  },\n",
            "  \"vision_config_dict\": null\n",
            "}\n",
            "\n",
            "loading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/model.safetensors\n",
            "All model checkpoint weights were used when initializing OwlViTForObjectDetection.\n",
            "\n",
            "All the weights of OwlViTForObjectDetection were initialized from the model checkpoint at google/owlvit-base-patch32.\n",
            "If your task is similar to the task the model of the checkpoint was trained on, you can already use OwlViTForObjectDetection for predictions without further training.\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/preprocessor_config.json\n",
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/preprocessor_config.json\n",
            "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got [768, 768]. Converted to {'height': 768, 'width': 768}.\n",
            "size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'longest_edge', 'shortest_edge'}, {'longest_edge'}), got 768. Converted to {'height': 768, 'width': 768}.\n",
            "Image processor OwlViTImageProcessor {\n",
            "  \"crop_size\": {\n",
            "    \"height\": 768,\n",
            "    \"width\": 768\n",
            "  },\n",
            "  \"do_center_crop\": false,\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"OwlViTImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"processor_class\": \"OwlViTProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 768,\n",
            "    \"width\": 768\n",
            "  }\n",
            "}\n",
            "\n",
            "loading file vocab.json from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/vocab.json\n",
            "loading file merges.txt from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/merges.txt\n",
            "loading file tokenizer.json from cache at None\n",
            "loading file added_tokens.json from cache at None\n",
            "loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/special_tokens_map.json\n",
            "loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--google--owlvit-base-patch32/snapshots/cbc355fb364588351c5d51c7f74465e8e7ec6f72/tokenizer_config.json\n",
            "ftfy or spacy is not installed using custom BasicTokenizer instead of ftfy.\n",
            "Processor OwlViTProcessor:\n",
            "- image_processor: OwlViTImageProcessor {\n",
            "  \"crop_size\": {\n",
            "    \"height\": 768,\n",
            "    \"width\": 768\n",
            "  },\n",
            "  \"do_center_crop\": false,\n",
            "  \"do_convert_rgb\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"image_mean\": [\n",
            "    0.48145466,\n",
            "    0.4578275,\n",
            "    0.40821073\n",
            "  ],\n",
            "  \"image_processor_type\": \"OwlViTImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.26862954,\n",
            "    0.26130258,\n",
            "    0.27577711\n",
            "  ],\n",
            "  \"processor_class\": \"OwlViTProcessor\",\n",
            "  \"resample\": 3,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"height\": 768,\n",
            "    \"width\": 768\n",
            "  }\n",
            "}\n",
            "\n",
            "- tokenizer: CLIPTokenizerFast(name_or_path='google/owlvit-base-patch32', vocab_size=49408, model_max_length=16, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '<|startoftext|>', 'eos_token': '<|endoftext|>', 'unk_token': '<|endoftext|>', 'pad_token': '!'}, clean_up_tokenization_spaces=True),  added_tokens_decoder={\n",
            "\t0: AddedToken(\"!\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
            "\t49406: AddedToken(\"<|startoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "\t49407: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=True, special=True),\n",
            "}\n",
            "\n",
            "{\n",
            "  \"processor_class\": \"OwlViTProcessor\"\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import torch\n",
        "from PIL import Image\n",
        "from transformers import *\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoModelForZeroShotObjectDetection\n",
        "from transformers import AutoProcessor, OwlViTForObjectDetection\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "from transformers import AutoProcessor, OwlViTForObjectDetection\n",
        "\n",
        "#model = OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "model = AutoModelForZeroShotObjectDetection.from_pretrained(\"google/owlvit-base-patch32\")\n",
        "\n",
        "processor = AutoProcessor.from_pretrained(\"google/owlvit-base-patch32\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "suO4z5NCDU8v"
      },
      "source": [
        "# Loading our Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "SlxPT1XaD9VV"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "# download & load the dataset\n",
        "ds = load_dataset(\"rathi2023/owlvitnhoodfinal\")\n",
        "ds =  ds[\"train\"].select(range(30))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QVjb_p9kDr5Q"
      },
      "source": [
        "# Exploring the Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "96AVmYAo9SCY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4235a56-0cd7-459d-fcb8-7c3d39adb9ab"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset({\n",
            "    features: ['image', 'image_id', 'objects', 'text_input'],\n",
            "    num_rows: 30\n",
            "})\n"
          ]
        }
      ],
      "source": [
        "print(ds)\n",
        "ds = ds.train_test_split(test_size=0.10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W8jdhJCQKCOA"
      },
      "source": [
        "# Preprocessing the Data"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = \"google/owlvit-base-patch32\"\n",
        "\n",
        "from transformers import OwlViTProcessor, OwlViTForObjectDetection\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n"
      ],
      "metadata": {
        "id": "OEFFj1tCMoEY",
        "outputId": "370b089f-0901-4c68-bd5c-d410919d6cb7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--facebook--detr-resnet-50/snapshots/1d5f47bd3bdd2c4bbfa585418ffe6da5028b4c0b/preprocessor_config.json\n",
            "Image processor DetrImageProcessor {\n",
            "  \"do_convert_annotations\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_pad\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"format\": \"coco_detection\",\n",
            "  \"image_mean\": [\n",
            "    0.485,\n",
            "    0.456,\n",
            "    0.406\n",
            "  ],\n",
            "  \"image_processor_type\": \"DetrImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.229,\n",
            "    0.224,\n",
            "    0.225\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 1333,\n",
            "    \"shortest_edge\": 800\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "C8hMr8XpIH2X",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0bc474e2-2a7e-42d9-8a4b-0f371e9bdab9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "loading configuration file preprocessor_config.json from cache at /root/.cache/huggingface/hub/models--facebook--detr-resnet-50/snapshots/1d5f47bd3bdd2c4bbfa585418ffe6da5028b4c0b/preprocessor_config.json\n",
            "Image processor DetrImageProcessor {\n",
            "  \"do_convert_annotations\": true,\n",
            "  \"do_normalize\": true,\n",
            "  \"do_pad\": true,\n",
            "  \"do_rescale\": true,\n",
            "  \"do_resize\": true,\n",
            "  \"format\": \"coco_detection\",\n",
            "  \"image_mean\": [\n",
            "    0.485,\n",
            "    0.456,\n",
            "    0.406\n",
            "  ],\n",
            "  \"image_processor_type\": \"DetrImageProcessor\",\n",
            "  \"image_std\": [\n",
            "    0.229,\n",
            "    0.224,\n",
            "    0.225\n",
            "  ],\n",
            "  \"resample\": 2,\n",
            "  \"rescale_factor\": 0.00392156862745098,\n",
            "  \"size\": {\n",
            "    \"longest_edge\": 1333,\n",
            "    \"shortest_edge\": 800\n",
            "  }\n",
            "}\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import albumentations\n",
        "import numpy as np\n",
        "from datasets import Dataset\n",
        "import pandas as pd\n",
        "# Apply transformations to the modified train dataset\n",
        "import copy\n",
        "\n",
        "\n",
        "# download & load the dataset\n",
        "# Add a new column 'image_id' with unique identifiers\n",
        "num_rows = len(ds)\n",
        "unique_image_ids = np.arange(num_rows)\n",
        "\n",
        "# Shuffle the unique image IDs to ensure randomness\n",
        "np.random.shuffle(unique_image_ids)\n",
        "\n",
        "from transformers import AutoImageProcessor\n",
        "from transformers import AutoProcessor\n",
        "\n",
        "image_processor = AutoImageProcessor.from_pretrained(\"facebook/detr-resnet-50\")\n",
        "\n",
        "import albumentations\n",
        "import numpy as np\n",
        "\n",
        "transform = albumentations.Compose(\n",
        "    [\n",
        "        albumentations.Resize(480, 480),\n",
        "        albumentations.HorizontalFlip(p=1.0),\n",
        "        albumentations.RandomBrightnessContrast(p=1.0),\n",
        "    ],\n",
        "    bbox_params=albumentations.BboxParams(format=\"coco\", label_fields=[\"category\"]),\n",
        ")\n",
        "def formatted_anns(image_id, category, area, bbox):\n",
        "    annotations = []\n",
        "    for i in range(0, len(category)):\n",
        "        new_ann = {\n",
        "            \"image_id\": image_id,\n",
        "            \"category_id\": category[i],\n",
        "            \"isCrowd\": 0,\n",
        "            \"area\": area,\n",
        "            \"bbox\": list(bbox[i]),\n",
        "        }\n",
        "        annotations.append(new_ann)\n",
        "\n",
        "    return annotations\n",
        "\n",
        "\n",
        "# transforming a batch\n",
        "\n",
        "# transforming a batch\n",
        "# transforming a batch\n",
        "def transform_aug_ann(examples):\n",
        "    image_ids = examples[\"image_id\"]\n",
        "    images, bboxes, area, categories = [], [], [], []\n",
        "    transformed_data = []\n",
        "    max_augmentation = 2\n",
        "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
        "        iter = 0\n",
        "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
        "        while iter < max_augmentation:\n",
        "          out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category_id\"])\n",
        "          last_two_values = objects[\"bbox\"][0][-2:]\n",
        "\n",
        "    # Unpack the values into separate variables\n",
        "          width, height = last_two_values\n",
        "\n",
        "          area.append(width * height)\n",
        "          area.append(area)\n",
        "          images.append(out[\"image\"])\n",
        "          bboxes.append(out[\"bboxes\"])\n",
        "          categories.append(out[\"category\"])\n",
        "        transformed_data.append(processor(text=examples[\"text_input\"], images=image,return_tensors=\"pt\", padding=True,truncation=True))\n",
        "\n",
        "\n",
        "    return {\"transformed_data\":transformed_data}\n",
        "\n",
        "def transform_aug_ann_labels(examples):\n",
        "    image_ids = examples[\"image_id\"]\n",
        "    images, bboxes, area, categories = [], [], [], []\n",
        "    max_augmentation = 2\n",
        "    for image, objects in zip(examples[\"image\"], examples[\"objects\"]):\n",
        "        iter = 0\n",
        "        image = np.array(image.convert(\"RGB\"))[:, :, ::-1]\n",
        "        while iter < max_augmentation:\n",
        "          out = transform(image=image, bboxes=objects[\"bbox\"], category=objects[\"category_id\"])\n",
        "\n",
        "          last_two_values = objects[\"bbox\"][0][-2:]\n",
        "\n",
        "    # Unpack the values into separate variables\n",
        "          width, height = last_two_values\n",
        "\n",
        "          area.append(width * height)\n",
        "          images.append(out[\"image\"])\n",
        "          bboxes.append(out[\"bboxes\"])\n",
        "          categories.append(out[\"category\"])\n",
        "          iter += 1\n",
        "\n",
        "    targets = [\n",
        "        {\"image_id\": id_, \"annotations\": formatted_anns(id_, cat_, ar_, box_)}\n",
        "        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)\n",
        "    ]\n",
        "\n",
        "    return image_processor(images=images, annotations=targets, return_tensors=\"pt\")\n",
        "\n",
        "# Preprocessed Training Data\n",
        "\n",
        "import pandas as pd\n",
        "from datasets import Dataset\n",
        "# https://huggingface.co/docs/datasets/en/process\n",
        "# Process dataset\n",
        "transform_1 = ds[\"train\"].with_transform(transform_aug_ann).shuffle(seed=42)\n",
        "transform_2 = ds[\"train\"].with_transform(transform_aug_ann_labels).shuffle(seed=42)\n",
        "data = []\n",
        "for i in range(len(transform_1)):\n",
        "    dict_ = {}\n",
        "    dict_[\"input_ids\"] = transform_1[i][\"transformed_data\"][\"input_ids\"]\n",
        "    dict_[\"attention_mask\"] = transform_1[i][\"transformed_data\"][\"attention_mask\"]\n",
        "    dict_[\"pixel_values\"] = transform_1[i][\"transformed_data\"][\"pixel_values\"][0]\n",
        "    dict_[\"labels\"] = transform_2[i][\"labels\"]\n",
        "    data.append(dict_)\n",
        "\n",
        "# Preprocessed Training Data\n",
        "train_dataset = Dataset.from_list(data)\n",
        "train_dataset.features\n",
        "\n",
        "print(ds[\"train\"][0])\n",
        "# Using Detr-Loss calculation https://github.com/facebookresearch/detr/blob/main/models/matcher.py\n",
        "# https://www.kaggle.com/code/bibhasmondal96/detr-from-scratch\n",
        "class BoxUtils(object):\n",
        "    @staticmethod\n",
        "    def box_cxcywh_to_xyxy(x):\n",
        "        x_c, y_c, w, h = x.unbind(-1)\n",
        "        b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "             (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "        return torch.stack(b, dim=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def box_xyxy_to_cxcywh(x):\n",
        "        x0, y0, x1, y1 = x.unbind(-1)\n",
        "        b = [(x0 + x1) / 2, (y0 + y1) / 2,\n",
        "             (x1 - x0), (y1 - y0)]\n",
        "        return torch.stack(b, dim=-1)\n",
        "\n",
        "    @staticmethod\n",
        "    def rescale_bboxes(out_bbox, size):\n",
        "        img_h, img_w = size\n",
        "        b = BoxUtils.box_cxcywh_to_xyxy(out_bbox)\n",
        "        b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "        return b\n",
        "\n",
        "    @staticmethod\n",
        "    def box_area(boxes):\n",
        "        \"\"\"\n",
        "        Computes the area of a set of bounding boxes, which are specified by its\n",
        "        (x1, y1, x2, y2) coordinates.\n",
        "        Arguments:\n",
        "            boxes (Tensor[N, 4]): boxes for which the area will be computed. They\n",
        "                are expected to be in (x1, y1, x2, y2) format\n",
        "        Returns:\n",
        "            area (Tensor[N]): area for each box\n",
        "        \"\"\"\n",
        "        return (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
        "\n",
        "    @staticmethod\n",
        "    # modified from torchvision to also return the union\n",
        "    def box_iou(boxes1, boxes2):\n",
        "        area1 = BoxUtils.box_area(boxes1)\n",
        "        area2 = BoxUtils.box_area(boxes2)\n",
        "\n",
        "        lt = torch.max(boxes1[:, None, :2], boxes2[:, :2])  # [N,M,2]\n",
        "        rb = torch.min(boxes1[:, None, 2:], boxes2[:, 2:])  # [N,M,2]\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "        inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\n",
        "\n",
        "        union = area1[:, None] + area2 - inter\n",
        "\n",
        "        iou = inter / union\n",
        "        return iou, union\n",
        "\n",
        "    @staticmethod\n",
        "    def generalized_box_iou(boxes1, boxes2):\n",
        "        \"\"\"\n",
        "        Generalized IoU from https://giou.stanford.edu/\n",
        "        The boxes should be in [x0, y0, x1, y1] format\n",
        "        Returns a [N, M] pairwise matrix, where N = len(boxes1)\n",
        "        and M = len(boxes2)\n",
        "        \"\"\"\n",
        "        # degenerate boxes gives inf / nan results\n",
        "        # so do an early check\n",
        "        assert (boxes1[:, 2:] >= boxes1[:, :2]).all()\n",
        "        assert (boxes2[:, 2:] >= boxes2[:, :2]).all()\n",
        "        iou, union = BoxUtils.box_iou(boxes1, boxes2)\n",
        "\n",
        "        lt = torch.min(boxes1[:, None, :2], boxes2[:, :2])\n",
        "        rb = torch.max(boxes1[:, None, 2:], boxes2[:, 2:])\n",
        "\n",
        "        wh = (rb - lt).clamp(min=0)  # [N,M,2]\n",
        "        area = wh[:, :, 0] * wh[:, :, 1]\n",
        "\n",
        "        return iou - (area - union) / area"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Extract the labels from the \"objects\" field\n",
        "train_labels = []\n",
        "for example in ds[\"train\"]:\n",
        "    objects = example[\"objects\"][\"category_id\"]\n",
        "    train_labels.extend(objects)\n",
        "# Create a set of unique labels\n",
        "unique_labels = set(train_labels)\n",
        "train_labels.sort()\n",
        "# Create a dictionary to map each unique label to a unique integer ID\n",
        "label2id = {label: idx for idx, label in enumerate(unique_labels)}"
      ],
      "metadata": {
        "id": "V8MPbhFdkvDJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hj3KEbI-Y4hP"
      },
      "outputs": [],
      "source": [
        "from transformers import OwlViTForObjectDetection, OwlViTFeatureExtractor\n",
        "from transformers import TrainingArguments, Trainer\n",
        "import torch\n",
        "model= OwlViTForObjectDetection.from_pretrained(\"google/owlvit-base-patch32\",\n",
        "    num_labels=len(unique_labels),\n",
        "    id2label={str(i): c for i, c in enumerate(unique_labels)},\n",
        "    label2id={c: str(i) for i, c in enumerate(unique_labels)},\n",
        "    ignore_mismatched_sizes=True\n",
        "\n",
        ")\n",
        "model.to(device)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mNT_iBYyKGAE"
      },
      "source": [
        "# Defining the Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "crXIbHCeJYFs"
      },
      "outputs": [],
      "source": [
        "from evaluate import load\n",
        "import numpy as np\n",
        "\n",
        "# load the accuracy and f1 metrics from the evaluate module\n",
        "accuracy = load(\"accuracy\")\n",
        "f1 = load(\"f1\")\n",
        "\n",
        "def compute_metrics(eval_pred):\n",
        "  # compute the accuracy and f1 scores & return them\n",
        "  accuracy_score = accuracy.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids)\n",
        "  f1_score = f1.compute(predictions=np.argmax(eval_pred.predictions, axis=1), references=eval_pred.label_ids, average=\"macro\")\n",
        "  return {**accuracy_score, **f1_score}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2WNJaZeYKJZH"
      },
      "source": [
        "# Training the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EVzsvp_Qxo85"
      },
      "outputs": [],
      "source": [
        "def collate_fn(batch):\n",
        "\n",
        "    lbps=[]\n",
        "    input_ids = torch.Tensor([item[\"input_ids\"] for item in batch]).int()\n",
        "    input_ids = input_ids.to(device)\n",
        "    # attention_mask = torch.Tensor([item[\"attention_mask\"].tolist() for item in batch]).int()\n",
        "    attention_mask = torch.Tensor([item[\"attention_mask\"] for item in batch]).int()\n",
        "    attention_mask = attention_mask.to(device)\n",
        "    # pixel_values = torch.Tensor([item[\"pixel_values\"].tolist() for item in batch])\n",
        "    pixel_values = torch.Tensor([item[\"pixel_values\"] for item in batch])\n",
        "    pixel_values = pixel_values.to(device)\n",
        "\n",
        "    labels = []\n",
        "    for item in batch:\n",
        "        for (key, value) in item[\"labels\"].items():\n",
        "            item[\"labels\"][key] = torch.Tensor(value).to(device)\n",
        "        labels.append(item[\"labels\"])\n",
        "\n",
        "    # Return the batch with stacked tensors\n",
        "    return {\n",
        "        \"input_ids\": input_ids,\n",
        "        \"pixel_values\": pixel_values,\n",
        "        \"labels\": labels,\n",
        "        \"attention_mask\": attention_mask,\n",
        "    }\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RNnG1GUUK7nj"
      },
      "outputs": [],
      "source": [
        "\n",
        "# for owlvit\n",
        "\n",
        "from transformers import TrainingArguments\n",
        "import os\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    \"owlvit-base-patch32_FT_cppe5\",\n",
        "    per_device_train_batch_size=1,\n",
        "    num_train_epochs=10,\n",
        "    fp16=False,\n",
        "    save_steps=200,\n",
        "    logging_steps=50,\n",
        "    learning_rate=1e-5,\n",
        "    weight_decay=1e-4,\n",
        "    save_total_limit=2,\n",
        "    remove_unused_columns=False,\n",
        "    push_to_hub=True,\n",
        "    dataloader_pin_memory=False,\n",
        "    gradient_accumulation_steps=1\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GE8E8sL3BmTh"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import notebook_login\n",
        "\n",
        "notebook_login()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1No4kyLhYfox"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "from scipy.optimize import linear_sum_assignment\n",
        "\n",
        "class HungarianMatcher(nn.Module):\n",
        "    \"\"\"This class computes an assignment between the targets and the predictions of the network\n",
        "    For efficiency reasons, the targets don't include the no_object. Because of this, in general,\n",
        "    there are more predictions than targets. In this case, we do a 1-to-1 matching of the best predictions,\n",
        "    while the others are un-matched (and thus treated as non-objects).\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, cost_class: float = 1, cost_bbox: float = 1, cost_giou: float = 1):\n",
        "        \"\"\"Creates the matcher\n",
        "        Params:\n",
        "            cost_class: This is the relative weight of the classification error in the matching cost\n",
        "            cost_bbox: This is the relative weight of the L1 error of the bounding box coordinates in the matching cost\n",
        "            cost_giou: This is the relative weight of the giou loss of the bounding box in the matching cost\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.cost_class = cost_class\n",
        "        self.cost_bbox = cost_bbox\n",
        "        self.cost_giou = cost_giou\n",
        "        assert cost_class != 0 or cost_bbox != 0 or cost_giou != 0, \"all costs cant be 0\"\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" Performs the matching\n",
        "        Params:\n",
        "            outputs: This is a dict that contains at least these entries:\n",
        "                 \"pred_logits\": Tensor of dim [batch_size, num_queries, num_classes] with the classification logits\n",
        "                 \"pred_boxes\": Tensor of dim [batch_size, num_queries, 4] with the predicted box coordinates\n",
        "            targets: This is a list of targets (len(targets) = batch_size), where each target is a dict containing:\n",
        "                 \"labels\": Tensor of dim [num_target_boxes] (where num_target_boxes is the number of ground-truth\n",
        "                           objects in the target) containing the class labels\n",
        "                 \"boxes\": Tensor of dim [num_target_boxes, 4] containing the target box coordinates\n",
        "        Returns:\n",
        "            A list of size batch_size, containing tuples of (index_i, index_j) where:\n",
        "                - index_i is the indices of the selected predictions (in order)\n",
        "                - index_j is the indices of the corresponding selected targets (in order)\n",
        "            For each batch element, it holds:\n",
        "                len(index_i) = len(index_j) = min(num_queries, num_target_boxes)\n",
        "        \"\"\"\n",
        "        bs, num_queries = outputs[\"logits\"].shape[:2]\n",
        "\n",
        "        # We flatten to compute the cost matrices in a batch\n",
        "        out_prob = outputs[\"logits\"].flatten(0, 1).softmax(-1)  # [batch_size * num_queries, num_classes]\n",
        "        out_bbox = outputs[\"pred_boxes\"].flatten(0, 1)  # [batch_size * num_queries, 4]\n",
        "\n",
        "        out_prob = torch.randn(576, 1)\n",
        "\n",
        "        # Expand out_prob along the second dimension to have shape [576, 47]\n",
        "        out_prob= out_prob.expand(-1, len(train_labels))\n",
        "        # Also concat the target labels and boxes\n",
        "        tgt_ids = torch.cat([v[\"class_labels\"] for v in targets])\n",
        "\n",
        "\n",
        "        tgt_ids = tgt_ids.int()\n",
        "\n",
        "\n",
        "        tgt_bbox = torch.cat([v[\"boxes\"] for v in targets])\n",
        "        out_prob=out_prob.to(device)\n",
        "\n",
        "        # Compute the classification cost. Contrary to the loss, we don't use the NLL,\n",
        "        # but approximate it in 1 - proba[target class].\n",
        "        # The 1 is a constant that doesn't change the matching, it can be ommitted.\n",
        "        cost_class = -out_prob[:, tgt_ids]\n",
        "\n",
        "        # Compute the L1 cost between boxes\n",
        "        cost_bbox = torch.cdist(out_bbox, tgt_bbox, p=1)\n",
        "\n",
        "        # Compute the giou cost betwen boxes\n",
        "        cost_giou = -BoxUtils.generalized_box_iou(\n",
        "            BoxUtils.box_cxcywh_to_xyxy(out_bbox),\n",
        "            BoxUtils.box_cxcywh_to_xyxy(tgt_bbox)\n",
        "        )\n",
        "\n",
        "        # Final cost matrix\n",
        "        C = self.cost_bbox * cost_bbox + self.cost_class * cost_class + self.cost_giou * cost_giou\n",
        "        C = C.view(bs, num_queries, -1).cpu()\n",
        "\n",
        "        sizes = [len(v[\"boxes\"]) for v in targets]\n",
        "        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(sizes, -1))]\n",
        "        return [(torch.as_tensor(i, dtype=torch.int64), torch.as_tensor(j, dtype=torch.int64)) for i, j in indices]\n",
        "\n",
        "class SetCriterion(nn.Module):\n",
        "    \"\"\" This class computes the loss for DETR.\n",
        "    The process happens in two steps:\n",
        "        1) we compute hungarian assignment between ground truth boxes and the outputs of the model\n",
        "        2) we supervise each pair of matched ground-truth / prediction (supervise class and box)\n",
        "    \"\"\"\n",
        "    def __init__(self, num_classes, matcher, weight_dict, eos_coef, losses):\n",
        "        \"\"\" Create the criterion.\n",
        "        Parameters:\n",
        "            num_classes: number of object categories, omitting the special no-object category\n",
        "            matcher: module able to compute a matching between targets and proposals\n",
        "            weight_dict: dict containing as key the names of the losses and as values their relative weight.\n",
        "            eos_coef: relative classification weight applied to the no-object category\n",
        "            losses: list of all the losses to be applied. See get_loss for list of available losses.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.num_classes = num_classes\n",
        "        self.matcher = matcher\n",
        "        self.weight_dict = weight_dict\n",
        "        self.eos_coef = eos_coef\n",
        "        self.losses = losses\n",
        "        empty_weight = torch.ones(self.num_classes + 1)\n",
        "        empty_weight[-1] = self.eos_coef\n",
        "        self.register_buffer('empty_weight', empty_weight)\n",
        "\n",
        "    def loss_labels(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Classification loss (NLL)\n",
        "        targets dicts must contain the key \"labels\" containing a tensor of dim [nb_target_boxes]\n",
        "        \"\"\"\n",
        "        assert 'logits' in outputs\n",
        "        src_logits = outputs['logits']\n",
        "\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        target_classes_o = torch.cat([t[\"class_labels\"][J] for t, (_, J) in zip(targets, indices)]).to(torch.int64)\n",
        "        target_classes = torch.full(src_logits.shape[:2], self.num_classes,\n",
        "                                    dtype=torch.int64, device=src_logits.device).to(torch.int64)\n",
        "        target_classes[idx] = target_classes_o\n",
        "\n",
        "\n",
        "\n",
        "        class_weights = compute_class_weight(\n",
        "                                        class_weight = \"balanced\",\n",
        "                                        classes = np.unique(train_labels),\n",
        "                                        y = train_labels\n",
        "                                    )\n",
        "        weights= torch.tensor(class_weights,dtype=torch.float)\n",
        "\n",
        "# push to GPU\n",
        "        weights = weights.to(device)\n",
        "\n",
        "        self.empty_weight=self.empty_weight.to(device)\n",
        "        loss_ce = F.cross_entropy(src_logits.transpose(1, 2),target_classes,reduction='none')\n",
        "\n",
        "\n",
        "        losses = {'loss_ce': loss_ce}\n",
        "        return losses\n",
        "\n",
        "    @torch.no_grad()\n",
        "    def loss_cardinality(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\" Compute the cardinality error, ie the absolute error in the number of predicted non-empty boxes\n",
        "        This is not really a loss, it is intended for logging purposes only. It doesn't propagate gradients\n",
        "        \"\"\"\n",
        "        pred_logits = outputs['logits']\n",
        "        device = pred_logits.device\n",
        "        tgt_lengths = torch.as_tensor([len(v[\"class_labels\"]) for v in targets], device=device)\n",
        "        # Count the number of predictions that are NOT \"no-object\" (which is the last class)\n",
        "        card_pred = (pred_logits.argmax(-1) != pred_logits.shape[-1] - 1).sum(1)\n",
        "        card_err = F.l1_loss(card_pred.float(), tgt_lengths.float())\n",
        "        losses = {'cardinality_error': card_err}\n",
        "        return losses\n",
        "\n",
        "    def loss_boxes(self, outputs, targets, indices, num_boxes):\n",
        "        \"\"\"Compute the losses related to the bounding boxes, the L1 regression loss and the GIoU loss\n",
        "           targets dicts must contain the key \"boxes\" containing a tensor of dim [nb_target_boxes, 4]\n",
        "           The target boxes are expected in format (center_x, center_y, w, h), normalized by the image size.\n",
        "        \"\"\"\n",
        "        assert 'pred_boxes' in outputs\n",
        "        idx = self._get_src_permutation_idx(indices)\n",
        "        src_boxes = outputs['pred_boxes'][idx]\n",
        "        target_boxes = torch.cat([t['boxes'][i] for t, (_, i) in zip(targets, indices)], dim=0)\n",
        "\n",
        "        loss_bbox = F.l1_loss(src_boxes, target_boxes, reduction='none')\n",
        "\n",
        "        losses = {}\n",
        "        losses['loss_bbox'] = loss_bbox.sum() / num_boxes\n",
        "\n",
        "        loss_giou = 1 - torch.diag(BoxUtils.generalized_box_iou(\n",
        "            BoxUtils.box_cxcywh_to_xyxy(src_boxes),\n",
        "            BoxUtils.box_cxcywh_to_xyxy(target_boxes))\n",
        "        )\n",
        "        losses['loss_giou'] = loss_giou.sum() / num_boxes\n",
        "        return losses\n",
        "\n",
        "    def _get_src_permutation_idx(self, indices):\n",
        "        # permute predictions following indices\n",
        "        batch_idx = torch.cat([torch.full_like(src, i) for i, (src, _) in enumerate(indices)])\n",
        "        src_idx = torch.cat([src for (src, _) in indices])\n",
        "        return batch_idx, src_idx\n",
        "\n",
        "    def _get_tgt_permutation_idx(self, indices):\n",
        "        # permute targets following indices\n",
        "        batch_idx = torch.cat([torch.full_like(tgt, i) for i, (_, tgt) in enumerate(indices)])\n",
        "        tgt_idx = torch.cat([tgt for (_, tgt) in indices])\n",
        "        return batch_idx, tgt_idx\n",
        "\n",
        "    def get_loss(self, loss, outputs, targets, indices, num_boxes, **kwargs):\n",
        "        loss_map = {\n",
        "            'labels': self.loss_labels,\n",
        "            'cardinality': self.loss_cardinality,\n",
        "            'boxes': self.loss_boxes,\n",
        "        }\n",
        "        assert loss in loss_map, f'do you really want to compute {loss} loss?'\n",
        "        return loss_map[loss](outputs, targets, indices, num_boxes, **kwargs)\n",
        "\n",
        "    def forward(self, outputs, targets):\n",
        "        \"\"\" This performs the loss computation.\n",
        "        Parameters:\n",
        "             outputs: dict of tensors, see the output specification of the model for the format\n",
        "             targets: list of dicts, such that len(targets) == batch_size.\n",
        "                      The expected keys in each dict depends on the losses applied, see each loss' doc\n",
        "        \"\"\"\n",
        "        outputs_without_aux = {k: v for k, v in outputs.items() if k != 'aux_outputs'}\n",
        "\n",
        "        # Retrieve the matching between the outputs of the last layer and the targets\n",
        "        indices = self.matcher(outputs_without_aux, targets)\n",
        "\n",
        "        # Compute the average number of target boxes accross all nodes, for normalization purposes\n",
        "        num_boxes = sum(len(t[\"class_labels\"]) for t in targets)\n",
        "        num_boxes = torch.as_tensor([num_boxes], dtype=torch.float, device=next(iter(outputs.values())).device)\n",
        "\n",
        "        # Compute all the requested losses\n",
        "        losses = {}\n",
        "        for loss in self.losses:\n",
        "            losses.update(self.get_loss(loss, outputs, targets, indices, num_boxes))\n",
        "\n",
        "        return losses\n",
        "\n",
        "\n",
        "# custom loss\n",
        "def custom_loss(logits, labels):\n",
        "    num_classes = len(unique_labels)\n",
        "    matcher = HungarianMatcher(cost_class = 1, cost_bbox = 5, cost_giou = 2)\n",
        "    weight_dict = {'loss_ce': 1, 'loss_bbox': 5, 'loss_giou': 2}\n",
        "    losses = ['labels', 'boxes', 'cardinality']\n",
        "    criterion = SetCriterion(num_classes, matcher=matcher, weight_dict=weight_dict, eos_coef=0.1, losses=losses)\n",
        "    #criterion.to(device)\n",
        "    loss = criterion(logits, labels)\n",
        "\n",
        "    simu =loss.values()\n",
        "    scalar_values = []\n",
        "\n",
        "    for tensor in simu:\n",
        "      if torch.is_tensor(tensor):\n",
        "        if tensor.numel() == 1:  # Check if tensor has only one element\n",
        "          scalar_values.append(tensor.item())\n",
        "        else:\n",
        "          scalar_values.extend(tensor.flatten().tolist())  # Flatten the tensor and extract elements\n",
        "\n",
        "\n",
        "\n",
        "# Sum the scalar values\n",
        "    total_loss = sum(scalar_values)\n",
        "\n",
        "\n",
        "    return torch.tensor(total_loss,requires_grad=True)\n",
        "# subclass trainer\n",
        "from transformers import Trainer\n",
        "\n",
        "class CustomTrainer(Trainer):\n",
        "    def compute_loss(self, model, inputs, return_outputs=False):\n",
        "        labels = inputs.pop(\"labels\")\n",
        "\n",
        "        inputs[\"input_ids\"] = inputs[\"input_ids\"][0]\n",
        "        inputs[\"attention_mask\"] = inputs[\"attention_mask\"][0]\n",
        "        outputs = model(**inputs, return_dict=True)\n",
        "        loss = custom_loss(outputs, labels)\n",
        "        return (loss, outputs) if return_outputs else loss\n",
        "\n",
        "\n",
        "# use new trainer\n",
        "trainer = CustomTrainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    data_collator=collate_fn,\n",
        "    train_dataset=train_dataset,\n",
        "    tokenizer=processor\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wUUoQyh-QPED"
      },
      "outputs": [],
      "source": [
        "# start training\n",
        "trainer.train()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "machine_shape": "hm",
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}