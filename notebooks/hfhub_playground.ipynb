{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# install packages in libs (as editable)\n",
    "# !python -m pip install huggingface_hub==0.21.4\n",
    "# !python -m pip install safetensors==0.4.2\n",
    "# !python -m pip install -r ../requirements.txt\n",
    "# !python -m pip install -e ../libs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "config.json /Users/n0c09jf/.cache/huggingface/hub/models--google--owlv2-base-patch16-ensemble/snapshots/7c77fc1316a353da3109f81d80ae118dd58fd55b/config.json\n",
      "model.safetensors /Users/n0c09jf/.cache/huggingface/hub/models--google--owlv2-base-patch16-ensemble/snapshots/7c77fc1316a353da3109f81d80ae118dd58fd55b/model.safetensors\n",
      "preprocessor_config.json /Users/n0c09jf/.cache/huggingface/hub/models--google--owlv2-base-patch16-ensemble/snapshots/7c77fc1316a353da3109f81d80ae118dd58fd55b/preprocessor_config.json\n",
      "dict_keys(['box_head.dense0.bias', 'box_head.dense0.weight', 'box_head.dense1.bias', 'box_head.dense1.weight', 'box_head.dense2.bias', 'box_head.dense2.weight', 'class_head.dense0.bias', 'class_head.dense0.weight', 'class_head.logit_scale.bias', 'class_head.logit_scale.weight', 'class_head.logit_shift.bias', 'class_head.logit_shift.weight', 'layer_norm.bias', 'layer_norm.weight', 'objectness_head.dense0.bias', 'objectness_head.dense0.weight', 'objectness_head.dense1.bias', 'objectness_head.dense1.weight', 'objectness_head.dense2.bias', 'objectness_head.dense2.weight', 'owlv2.logit_scale', 'owlv2.text_model.embeddings.position_embedding.weight', 'owlv2.text_model.embeddings.token_embedding.weight', 'owlv2.text_model.encoder.layers.0.layer_norm1.bias', 'owlv2.text_model.encoder.layers.0.layer_norm1.weight', 'owlv2.text_model.encoder.layers.0.layer_norm2.bias', 'owlv2.text_model.encoder.layers.0.layer_norm2.weight', 'owlv2.text_model.encoder.layers.0.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.0.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.0.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.0.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.0.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.0.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.0.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.0.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.0.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.0.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.0.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.0.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.1.layer_norm1.bias', 'owlv2.text_model.encoder.layers.1.layer_norm1.weight', 'owlv2.text_model.encoder.layers.1.layer_norm2.bias', 'owlv2.text_model.encoder.layers.1.layer_norm2.weight', 'owlv2.text_model.encoder.layers.1.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.1.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.1.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.1.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.1.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.1.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.1.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.1.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.1.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.1.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.1.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.1.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.10.layer_norm1.bias', 'owlv2.text_model.encoder.layers.10.layer_norm1.weight', 'owlv2.text_model.encoder.layers.10.layer_norm2.bias', 'owlv2.text_model.encoder.layers.10.layer_norm2.weight', 'owlv2.text_model.encoder.layers.10.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.10.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.10.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.10.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.10.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.10.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.10.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.10.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.10.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.10.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.10.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.10.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.11.layer_norm1.bias', 'owlv2.text_model.encoder.layers.11.layer_norm1.weight', 'owlv2.text_model.encoder.layers.11.layer_norm2.bias', 'owlv2.text_model.encoder.layers.11.layer_norm2.weight', 'owlv2.text_model.encoder.layers.11.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.11.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.11.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.11.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.11.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.11.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.11.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.11.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.11.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.11.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.11.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.11.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.2.layer_norm1.bias', 'owlv2.text_model.encoder.layers.2.layer_norm1.weight', 'owlv2.text_model.encoder.layers.2.layer_norm2.bias', 'owlv2.text_model.encoder.layers.2.layer_norm2.weight', 'owlv2.text_model.encoder.layers.2.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.2.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.2.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.2.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.2.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.2.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.2.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.2.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.2.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.2.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.2.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.2.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.3.layer_norm1.bias', 'owlv2.text_model.encoder.layers.3.layer_norm1.weight', 'owlv2.text_model.encoder.layers.3.layer_norm2.bias', 'owlv2.text_model.encoder.layers.3.layer_norm2.weight', 'owlv2.text_model.encoder.layers.3.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.3.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.3.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.3.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.3.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.3.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.3.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.3.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.3.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.3.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.3.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.3.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.4.layer_norm1.bias', 'owlv2.text_model.encoder.layers.4.layer_norm1.weight', 'owlv2.text_model.encoder.layers.4.layer_norm2.bias', 'owlv2.text_model.encoder.layers.4.layer_norm2.weight', 'owlv2.text_model.encoder.layers.4.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.4.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.4.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.4.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.4.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.4.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.4.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.4.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.4.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.4.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.4.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.4.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.5.layer_norm1.bias', 'owlv2.text_model.encoder.layers.5.layer_norm1.weight', 'owlv2.text_model.encoder.layers.5.layer_norm2.bias', 'owlv2.text_model.encoder.layers.5.layer_norm2.weight', 'owlv2.text_model.encoder.layers.5.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.5.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.5.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.5.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.5.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.5.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.5.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.5.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.5.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.5.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.5.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.5.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.6.layer_norm1.bias', 'owlv2.text_model.encoder.layers.6.layer_norm1.weight', 'owlv2.text_model.encoder.layers.6.layer_norm2.bias', 'owlv2.text_model.encoder.layers.6.layer_norm2.weight', 'owlv2.text_model.encoder.layers.6.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.6.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.6.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.6.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.6.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.6.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.6.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.6.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.6.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.6.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.6.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.6.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.7.layer_norm1.bias', 'owlv2.text_model.encoder.layers.7.layer_norm1.weight', 'owlv2.text_model.encoder.layers.7.layer_norm2.bias', 'owlv2.text_model.encoder.layers.7.layer_norm2.weight', 'owlv2.text_model.encoder.layers.7.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.7.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.7.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.7.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.7.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.7.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.7.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.7.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.7.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.7.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.7.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.7.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.8.layer_norm1.bias', 'owlv2.text_model.encoder.layers.8.layer_norm1.weight', 'owlv2.text_model.encoder.layers.8.layer_norm2.bias', 'owlv2.text_model.encoder.layers.8.layer_norm2.weight', 'owlv2.text_model.encoder.layers.8.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.8.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.8.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.8.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.8.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.8.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.8.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.8.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.8.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.8.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.8.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.8.self_attn.v_proj.weight', 'owlv2.text_model.encoder.layers.9.layer_norm1.bias', 'owlv2.text_model.encoder.layers.9.layer_norm1.weight', 'owlv2.text_model.encoder.layers.9.layer_norm2.bias', 'owlv2.text_model.encoder.layers.9.layer_norm2.weight', 'owlv2.text_model.encoder.layers.9.mlp.fc1.bias', 'owlv2.text_model.encoder.layers.9.mlp.fc1.weight', 'owlv2.text_model.encoder.layers.9.mlp.fc2.bias', 'owlv2.text_model.encoder.layers.9.mlp.fc2.weight', 'owlv2.text_model.encoder.layers.9.self_attn.k_proj.bias', 'owlv2.text_model.encoder.layers.9.self_attn.k_proj.weight', 'owlv2.text_model.encoder.layers.9.self_attn.out_proj.bias', 'owlv2.text_model.encoder.layers.9.self_attn.out_proj.weight', 'owlv2.text_model.encoder.layers.9.self_attn.q_proj.bias', 'owlv2.text_model.encoder.layers.9.self_attn.q_proj.weight', 'owlv2.text_model.encoder.layers.9.self_attn.v_proj.bias', 'owlv2.text_model.encoder.layers.9.self_attn.v_proj.weight', 'owlv2.text_model.final_layer_norm.bias', 'owlv2.text_model.final_layer_norm.weight', 'owlv2.text_projection.weight', 'owlv2.vision_model.embeddings.class_embedding', 'owlv2.vision_model.embeddings.patch_embedding.weight', 'owlv2.vision_model.embeddings.position_embedding.weight', 'owlv2.vision_model.encoder.layers.0.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.0.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.0.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.0.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.0.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.0.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.0.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.0.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.0.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.0.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.0.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.0.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.0.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.0.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.0.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.0.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.1.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.1.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.1.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.1.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.1.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.1.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.1.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.1.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.1.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.1.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.1.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.1.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.1.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.1.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.1.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.1.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.10.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.10.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.10.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.10.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.10.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.10.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.10.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.10.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.10.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.10.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.10.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.10.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.10.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.10.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.10.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.10.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.11.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.11.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.11.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.11.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.11.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.11.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.11.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.11.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.11.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.11.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.11.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.11.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.11.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.11.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.11.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.11.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.2.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.2.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.2.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.2.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.2.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.2.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.2.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.2.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.2.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.2.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.2.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.2.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.2.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.2.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.2.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.2.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.3.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.3.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.3.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.3.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.3.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.3.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.3.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.3.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.3.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.3.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.3.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.3.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.3.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.3.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.3.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.3.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.4.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.4.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.4.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.4.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.4.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.4.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.4.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.4.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.4.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.4.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.4.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.4.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.4.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.4.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.4.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.4.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.5.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.5.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.5.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.5.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.5.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.5.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.5.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.5.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.5.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.5.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.5.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.5.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.5.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.5.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.5.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.5.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.6.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.6.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.6.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.6.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.6.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.6.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.6.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.6.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.6.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.6.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.6.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.6.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.6.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.6.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.6.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.6.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.7.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.7.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.7.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.7.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.7.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.7.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.7.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.7.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.7.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.7.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.7.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.7.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.7.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.7.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.7.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.7.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.8.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.8.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.8.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.8.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.8.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.8.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.8.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.8.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.8.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.8.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.8.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.8.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.8.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.8.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.8.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.8.self_attn.v_proj.weight', 'owlv2.vision_model.encoder.layers.9.layer_norm1.bias', 'owlv2.vision_model.encoder.layers.9.layer_norm1.weight', 'owlv2.vision_model.encoder.layers.9.layer_norm2.bias', 'owlv2.vision_model.encoder.layers.9.layer_norm2.weight', 'owlv2.vision_model.encoder.layers.9.mlp.fc1.bias', 'owlv2.vision_model.encoder.layers.9.mlp.fc1.weight', 'owlv2.vision_model.encoder.layers.9.mlp.fc2.bias', 'owlv2.vision_model.encoder.layers.9.mlp.fc2.weight', 'owlv2.vision_model.encoder.layers.9.self_attn.k_proj.bias', 'owlv2.vision_model.encoder.layers.9.self_attn.k_proj.weight', 'owlv2.vision_model.encoder.layers.9.self_attn.out_proj.bias', 'owlv2.vision_model.encoder.layers.9.self_attn.out_proj.weight', 'owlv2.vision_model.encoder.layers.9.self_attn.q_proj.bias', 'owlv2.vision_model.encoder.layers.9.self_attn.q_proj.weight', 'owlv2.vision_model.encoder.layers.9.self_attn.v_proj.bias', 'owlv2.vision_model.encoder.layers.9.self_attn.v_proj.weight', 'owlv2.vision_model.post_layernorm.bias', 'owlv2.vision_model.post_layernorm.weight', 'owlv2.vision_model.pre_layernorm.bias', 'owlv2.vision_model.pre_layernorm.weight', 'owlv2.visual_projection.weight'])\n"
     ]
    }
   ],
   "source": [
    "from huggingface_hub import hf_hub_download\n",
    "from safetensors import safe_open\n",
    "\n",
    "import json, torch\n",
    "\n",
    "cfg_file_path = hf_hub_download(repo_id=\"google/owlv2-base-patch16-ensemble\", filename=\"config.json\")\n",
    "print('config.json', cfg_file_path)\n",
    "\n",
    "model_file_path = hf_hub_download(repo_id=\"google/owlv2-base-patch16-ensemble\", filename=\"model.safetensors\")\n",
    "print('model.safetensors', model_file_path)\n",
    "\n",
    "preproc_file_path = hf_hub_download(repo_id=\"google/owlv2-base-patch16-ensemble\", filename=\"preprocessor_config.json\")\n",
    "print('preprocessor_config.json', preproc_file_path)\n",
    "\n",
    "state_dict = {}\n",
    "with safe_open(model_file_path, framework=\"pt\", device=\"cpu\") as f:\n",
    "   for key in f.keys():\n",
    "       state_dict[key] = f.get_tensor(key)\n",
    "print(state_dict.keys())\n",
    "\n",
    "with open(cfg_file_path, 'r') as f:\n",
    "    config = json.load(f)\n",
    "# print(json.dumps(config, indent=2))\n",
    "\n",
    "with open(preproc_file_path, 'r') as f:\n",
    "    preproc_config = json.load(f)\n",
    "# print(json.dumps(preproc_config, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Owlv2ForObjectDetection(\n",
      "  (vision_model): Owlv2VisionTransformer(\n",
      "    (embeddings): Owlv2VisionEmbeddings(\n",
      "      (patch_embedding): Conv2d(3, 768, kernel_size=(16, 16), stride=(16, 16), bias=False)\n",
      "      (position_embedding): Embedding(3601, 768)\n",
      "    )\n",
      "    (pre_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "    (encoder): Owlv2Encoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-11): 12 x Owlv2EncoderLayer(\n",
      "          (self_attn): Owlv2Attention(\n",
      "            (k_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (v_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (q_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "            (out_proj): Linear(in_features=768, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "          (mlp): Owlv2MLP(\n",
      "            (activation_fn): QuickGELUActivation()\n",
      "            (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
      "            (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
      "          )\n",
      "          (layer_norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (post_layernorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  )\n",
      "  (class_head): Owlv2ClassPredictionHead(\n",
      "    (dense0): Linear(in_features=768, out_features=512, bias=True)\n",
      "    (logit_shift): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (logit_scale): Linear(in_features=768, out_features=1, bias=True)\n",
      "    (elu): ELU(alpha=1.0)\n",
      "  )\n",
      "  (box_head): Owlv2BoxPredictionHead(\n",
      "    (dense0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dense1): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (gelu): GELU(approximate='none')\n",
      "    (dense2): Linear(in_features=768, out_features=4, bias=True)\n",
      "  )\n",
      "  (objectness_head): Owlv2BoxPredictionHead(\n",
      "    (dense0): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (dense1): Linear(in_features=768, out_features=768, bias=True)\n",
      "    (gelu): GELU(approximate='none')\n",
      "    (dense2): Linear(in_features=768, out_features=1, bias=True)\n",
      "  )\n",
      "  (layer_norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from binsense.owlv2 import Owlv2ForObjectDetection\n",
    "from binsense.owlv2.config import Owlv2Config\n",
    "from binsense.owlv2 import Owlv2ImageProcessor\n",
    "\n",
    "owl_processor = Owlv2ImageProcessor(**preproc_config)\n",
    "owl_config = Owlv2Config(**config)\n",
    "\n",
    "model = Owlv2ForObjectDetection(owl_config)\n",
    "model_module_keys = model.__dict__['_modules'].keys()\n",
    "appl_state_dict = dict()\n",
    "for key in state_dict.keys():\n",
    "    new_key = key\n",
    "    first_token = key.split('.')[0]\n",
    "    if first_token == 'owlv2':\n",
    "        new_key = '.'.join(key.split('.')[1:])\n",
    "        \n",
    "    if new_key.split('.')[0] in model_module_keys:\n",
    "        appl_state_dict[new_key] = state_dict[key]\n",
    "\n",
    "model.load_state_dict(appl_state_dict)\n",
    "print(model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "binsense_condaenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
